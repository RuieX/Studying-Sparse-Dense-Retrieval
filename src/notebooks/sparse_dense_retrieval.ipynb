{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup & Dataset Retrieval"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os.path\n",
    "import json\n",
    "import logging\n",
    "import pickle\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rank_bm25 as bm25\n",
    "from beir import util, LoggingHandler\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "\n",
    "import src.utilities.tokenization as tkn\n",
    "import src.utilities.scores as scr\n",
    "import src.utilities.evaluation as eva"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Followed beir tutorial to download dataset:\n",
    "# https://github.com/beir-cellar/beir#beers-quick-example\n",
    "\n",
    "# Show beir library logs in stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "\n",
    "DATASET_NAME = \"trec-covid\"\n",
    "DATASET_URL = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{DATASET_NAME}.zip\"\n",
    "DATA_DIR = \"../../data\"\n",
    "\n",
    "# Download only if necessary\n",
    "if not os.path.exists(os.path.join(DATA_DIR, DATASET_NAME)):\n",
    "    data_path = util.download_and_unzip(DATASET_URL, DATA_DIR)\n",
    "else:\n",
    "    data_path = os.path.join(DATA_DIR, DATASET_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-25 03:32:32 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/171332 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "faa1548193874560a0505334fdd99813"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-25 03:32:35 - Loaded 171332 TEST Documents.\n",
      "2023-04-25 03:32:35 - Doc Example: {'text': 'OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.', 'title': 'Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia'}\n",
      "2023-04-25 03:32:35 - Loading Queries...\n",
      "2023-04-25 03:32:36 - Loaded 50 TEST Queries.\n",
      "2023-04-25 03:32:36 - Query Example: what is the origin of COVID-19\n"
     ]
    }
   ],
   "source": [
    "# Load documents, queries and ground truth from the downloaded dataset.\n",
    "# Ground truth here is referred to as `_` because we do not care about it,\n",
    "#   part of the assignment is to calculate our own ground truth\n",
    "data: tkn.Dataset = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "documents, queries, _ = data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "171332"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# document & query cleaning and tokenization\n",
    "TOKENS_DIR = os.path.join(data_path, \"tokens\")\n",
    "if not os.path.exists(TOKENS_DIR):\n",
    "    os.mkdir(TOKENS_DIR)\n",
    "\n",
    "QUERY_TOKENS_PATH = os.path.join(TOKENS_DIR, \"query_tokens.pkl\")\n",
    "DOCS_TOKENS_PATH = os.path.join(TOKENS_DIR, \"doc_tokens.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing documents: 100%|██████████| 171332/171332 [13:06<00:00, 217.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[TokenizedText(text_id='ug7v899j', tokens=['clinical', 'features', 'culture', 'proven', '##co', '##pl', '##as', '##ma', 'pneumonia', '##e', 'infections', 'king', 'abdul', '##azi', '##z', 'university', 'hospital', 'jed', '##dah', 'saudi', 'arabia', 'objective', 'retrospective', 'chart', 'review', 'describes', 'ep', '##ide', '##mi', '##ology', 'clinical', 'features', '40', 'patients', 'culture', 'proven', '##co', '##pl', '##as', '##ma', 'pneumonia', '##e', 'infections', 'king', 'abdul', '##azi', '##z', 'university', 'hospital', 'jed', '##dah', 'saudi', 'arabia', 'methods', 'patients', 'positive', 'pneumonia', '##e', 'cultures', 'respiratory', 'specimens', 'january', '1997', 'december', '1998', 'identified', 'micro', '##biology', 'records', 'charts', 'patients', 'reviewed', 'results', '40', 'patients', 'identified', '33', '82', '5', 'required', 'admission', 'infections', '92', '5', 'community', 'acquired', 'infection', 'affected', 'age', 'groups', 'common', 'infants', '32', '5', 'pre', 'school', 'children', '22', '5', 'occurred', 'year', 'round', 'common', 'fall', '35', 'spring', '30', 'three', 'quarters', 'patients', '77', '5', 'como', '##rb', '##idi', '##ties', 'twenty', 'four', 'isolate', '##s', '60', 'associated', 'pneumonia', '14', '35', 'upper', 'respiratory', 'tract', 'infections', '2', '5', 'bro', '##nch', '##iol', '##itis', 'cough', '82', '5', 'fever', '75', 'mala', '##ise', '58', '8', 'common', 'symptoms', 'cr', '##ep', '##itation', '##s', '60', 'w', '##hee', '##zes', '40', 'common', 'signs', 'patients', 'pneumonia', 'cr', '##ep', '##itation', '##s', '79', '2', '25', 'bro', '##nch', '##ial', 'breathing', 'im', '##mun', '##oco', '##mp', '##rom', '##ised', 'patients', 'likely', 'non', 'im', '##mun', '##oco', '##mp', '##rom', '##ised', 'patients', 'present', 'pneumonia', '8', '9', 'versus', '16', '31', 'p', '0', '05', '24', 'patients', 'pneumonia', '14', '58', '3', 'uneven', '##tf', '##ul', 'recovery', '4', '16', '7', 'recovered', 'following', 'complications', '3', '12', '5', 'died', 'pneumonia', '##e', 'infection', '3', '12', '5', 'died', 'due', 'underlying', 'como', '##rb', '##idi', '##ties', '3', 'patients', 'died', 'pneumonia', '##e', 'pneumonia', 'como', '##rb', '##idi', '##ties', 'conclusion', 'results', 'similar', 'published', 'data', 'except', 'finding', 'infections', 'common', 'infants', 'preschool', 'children', 'mortality', 'rate', 'pneumonia', 'patients', 'como', '##rb', '##idi', '##ties', 'high']),\n TokenizedText(text_id='02tnwd4m', tokens=['ni', '##tric', 'oxide', 'pro', 'inflammatory', 'media', '##tor', 'lung', 'disease', 'inflammatory', 'diseases', 'respiratory', 'tract', 'commonly', 'associated', 'elevated', 'production', 'ni', '##tric', 'oxide', '•', 'increased', 'indices', '•', 'dependent', 'ox', '##ida', '##tive', 'stress', 'although', '•', 'known', 'anti', 'micro', '##bial', 'anti', 'inflammatory', 'anti', 'ox', '##ida', '##nt', 'properties', 'various', 'lines', 'evidence', 'support', 'contribution', '•', 'lung', 'injury', 'several', 'disease', 'models', 'basis', 'bio', '##chemical', 'evidence', 'often', 'presumed', '•', 'dependent', 'oxidation', '##s', 'due', 'formation', 'ox', '##ida', '##nt', 'per', '##ox', '##yn', '##it', '##rite', 'although', 'alternative', 'mechanisms', 'involving', 'ph', '##ago', '##cy', '##te', 'derived', 'hem', '##e', 'proteins', '##elo', '##per', '##ox', '##ida', '##se', 'e', '##osi', '##no', '##phi', '##l', 'per', '##ox', '##ida', '##se', 'might', 'operative', 'conditions', 'inflammation', 'overwhelming', 'literature', '•', 'generation', 'activities', 'respiratory', 'tract', 'would', 'beyond', 'scope', 'commentary', 'review', 'area', 'comprehensive', '##ly', 'instead', 'focuses', 'recent', 'evidence', 'concepts', 'presumed', 'contribution', '•', 'inflammatory', 'diseases', 'lung']),\n TokenizedText(text_id='ejv2xln0', tokens=['surf', '##act', '##ant', 'protein', 'pulmonary', 'host', 'defense', 'surf', '##act', '##ant', 'protein', 'sp', 'participates', 'innate', 'response', 'inhaled', 'micro', '##org', '##ani', '##sms', 'organic', 'antigen', '##s', 'contributes', 'immune', 'inflammatory', 'regulation', 'within', 'lung', 'sp', 'synthesized', 'secret', '##ed', 'al', '##ve', '##olar', 'bro', '##nch', '##iol', '##ar', 'ep', '##ith', '##elial', 'cells', 'also', 'expressed', 'ep', '##ith', '##elial', 'cells', 'lining', 'various', 'ex', '##oc', '##rine', 'duct', '##s', 'mu', '##cos', '##a', 'gas', '##tro', '##int', '##estinal', 'gen', '##ito', '##uri', '##nary', 'tracts', 'sp', 'col', '##lage', '##nous', 'calcium', 'dependent', 'le', '##ct', '##in', 'collect', '##in', 'binds', 'surface', 'g', '##ly', '##co', '##con', '##ju', '##gate', '##s', 'expressed', 'wide', 'variety', 'micro', '##org', '##ani', '##sms', 'ol', '##igo', '##sa', '##cc', '##hari', '##des', 'associated', 'surface', 'various', 'complex', 'organic', 'antigen', '##s', 'sp', 'also', 'specifically', 'interact', '##s', 'g', '##ly', '##co', '##con', '##ju', '##gate', '##s', 'molecules', 'expressed', 'surface', 'macro', '##pha', '##ges', 'ne', '##ut', '##rop', '##hil', '##s', 'l', '##ym', '##ph', '##ocytes', 'addition', 'sp', 'binds', 'specific', 'surf', '##act', '##ant', 'associated', 'lip', '##ids', 'influence', 'organization', 'lip', '##id', 'mixture', '##s', 'containing', 'ph', '##os', '##pha', '##ti', '##dy', '##lino', '##sit', '##ol', 'vitro', 'consistent', 'diverse', 'vitro', 'activities', 'observation', 'sp', 'def', '##icient', 'trans', '##genic', 'mice', 'show', 'abnormal', 'accumulation', '##s', 'surf', '##act', '##ant', 'lip', '##ids', 'respond', 'abnormal', '##ly', 'challenge', 'respiratory', 'viruses', 'bacterial', 'lip', '##op', '##ol', '##ys', '##ac', '##cha', '##ride', '##s', 'ph', '##eno', '##type', 'macro', '##pha', '##ges', 'isolated', 'lungs', 'sp', 'def', '##icient', 'mice', 'altered', 'ci', '##rc', '##ums', '##tan', '##tial', 'evidence', 'abnormal', 'ox', '##ida', '##nt', 'metabolism', 'increased', 'metal', '##lo', '##pro', '##tein', '##ase', 'expression', 'contributes', 'development', 'em', '##phy', '##se', '##ma', 'expression', 'sp', 'increased', 'response', 'many', 'forms', 'lung', 'injury', 'def', '##icient', 'accumulation', 'appropriately', 'ol', '##igo', '##mer', '##ized', 'sp', 'might', 'contribute', 'pathogen', '##esis', 'variety', 'human', 'lung', 'diseases']),\n TokenizedText(text_id='2b73a28n', tokens=['role', 'end', '##oth', '##elin', '1', 'lung', 'disease', 'end', '##oth', '##elin', '1', 'et', '1', '21', 'amino', 'acid', 'peptide', 'diverse', 'biological', 'activity', 'implicated', 'numerous', 'diseases', 'et', '1', 'potent', 'mit', '##ogen', 'regulator', 'smooth', 'muscle', 'tone', 'inflammatory', 'media', '##tor', 'may', 'play', 'key', 'role', 'diseases', 'airways', 'pulmonary', 'circulation', 'inflammatory', 'lung', 'diseases', 'acute', 'chronic', 'review', 'focus', 'biology', 'et', '1', 'role', 'lung', 'disease']),\n TokenizedText(text_id='9785vg6d', tokens=['gene', 'expression', 'ep', '##ith', '##elial', 'cells', 'response', 'p', '##ne', '##um', '##ov', '##irus', 'infection', 'respiratory', 'sync', '##yt', '##ial', 'virus', 'rs', '##v', 'pneumonia', 'virus', 'mice', 'pv', '##m', 'viruses', 'family', 'para', '##my', '##x', '##ov', '##iri', '##dae', 'subfamily', 'p', '##ne', '##um', '##ov', '##irus', 'cause', 'clinical', '##ly', 'important', 'respiratory', 'infections', 'humans', 'rodents', 'respectively', 'respiratory', 'ep', '##ith', '##elial', 'target', 'cells', 'respond', 'viral', 'infection', 'specific', 'alterations', 'gene', 'expression', 'including', 'production', 'che', '##mo', '##att', '##rac', '##tan', '##t', 'cy', '##tok', '##ines', 'ad', '##hesion', 'molecules', 'elements', 'related', 'ap', '##op', '##tosis', 'response', 'others', 'remain', 'incomplete', '##ly', 'understood', 'review', 'current', 'understanding', 'mu', '##cos', '##al', 'responses', 'discuss', 'several', 'gen', '##omic', 'approaches', 'including', 'differential', 'display', 'reverse', 'transcription', 'polymer', '##ase', 'chain', 'reaction', 'pc', '##r', 'gene', 'array', 'strategies', 'permit', 'us', 'un', '##rave', '##l', 'nature', 'responses', 'complete', 'systematic', 'manner']),\n TokenizedText(text_id='zjufx4fo', tokens=['sequence', 'requirements', 'rna', 'strand', 'transfer', 'ni', '##do', '##virus', 'disco', '##nti', '##nu', '##ous', 'sub', '##gen', '##omic', 'rna', 'synthesis', 'ni', '##do', '##virus', 'sub', '##gen', '##omic', 'mrna', '##s', 'contain', 'leader', 'sequence', 'derived', '5', '′', 'end', 'genome', 'fused', 'different', 'sequences', '‘', 'bodies', '’', 'derived', '3', '′', 'end', 'generation', 'involves', 'unique', 'mechanism', 'disco', '##nti', '##nu', '##ous', 'sub', '##gen', '##omic', 'rna', 'synthesis', 'resembles', 'copy', 'choice', 'rna', 'rec', '##om', '##bina', '##tion', 'process', 'nas', '##cent', 'rna', 'strand', 'transferred', 'one', 'site', 'template', 'another', 'either', 'plus', 'minus', 'strand', 'synthesis', 'yield', 'sub', '##gen', '##omic', 'rna', 'molecules', 'central', 'process', 'transcription', 'regulating', 'sequences', 'tr', '##ss', 'present', 'template', 'sites', 'ensure', 'fidelity', 'strand', 'transfer', 'present', 'results', 'comprehensive', 'co', 'variation', 'mu', '##tage', '##nes', '##is', 'study', 'e', '##quin', '##e', 'arte', '##rit', '##is', 'virus', 'tr', '##ss', 'demonstrating', 'disco', '##nti', '##nu', '##ous', 'rna', 'synthesis', 'depends', 'base', 'pairing', 'sense', 'leader', 'tr', '##s', 'anti', '##sen', '##se', 'body', 'tr', '##s', 'also', 'primary', 'sequence', 'body', 'tr', '##s', 'leader', 'tr', '##s', 'merely', 'plays', 'targeting', 'role', 'strand', 'transfer', 'body', 'tr', '##s', 'fu', '##lf', '##ils', 'multiple', 'functions', 'sequences', 'mrna', 'leader', '–', 'body', 'junctions', 'tr', '##s', 'mutants', 'strongly', 'suggested', 'disco', '##nti', '##nu', '##ous', 'step', 'occurs', 'minus', 'strand', 'synthesis']),\n TokenizedText(text_id='5yhe786e', tokens=['debate', 'trans', '##fus', '##ing', 'normal', 'ha', '##em', '##og', '##lo', '##bin', 'levels', 'improve', 'outcome', 'recent', 'evidence', 'suggests', 'critically', 'ill', 'patients', 'able', 'tolerate', 'lower', 'levels', 'ha', '##em', '##og', '##lo', '##bin', 'previously', 'believed', 'goal', 'show', 'trans', '##fus', '##ing', 'level', '100', 'g', 'l', 'improve', 'mortality', 'clinical', '##ly', 'important', 'outcomes', 'critical', 'care', 'setting', 'although', 'many', 'questions', 'remain', 'many', 'laboratory', 'clinical', 'studies', 'including', 'recent', 'random', '##ized', 'controlled', 'trial', 'rc', '##t', 'established', 'trans', '##fus', '##ing', 'normal', 'ha', '##em', '##og', '##lo', '##bin', 'concentrations', 'improve', 'organ', 'failure', 'mortality', 'critically', 'ill', 'patient', 'addition', 'restrictive', 'trans', '##fusion', 'strategy', 'reduce', 'exposure', '##ogen', '##ei', '##c', 'trans', '##fusion', '##s', 'result', 'efficient', 'use', 'red', 'blood', 'cells', 'rb', '##cs', 'save', 'blood', 'overall', 'decrease', 'health', 'care', 'costs']),\n TokenizedText(text_id='8zchiykl', tokens=['21st', 'international', 'symposium', 'intensive', 'care', 'emergency', 'medicine', 'brussels', 'belgium', '20', '23', 'march', '2001', '21st', 'international', 'symposium', 'intensive', 'care', 'emergency', 'medicine', 'dominated', 'results', 'recent', 'clinical', 'trials', 'sep', '##sis', 'acute', 'respiratory', 'distress', 'syndrome', 'ar', '##ds', 'promise', 'extra', '##corp', '##ore', '##al', 'liver', 'replacement', 'therapy', 'non', '##in', '##vas', '##ive', 'ventilation', 'areas', 'interest', 'ethical', 'issues', 'also', 'received', 'attention', 'overall', 'state', 'art', 'lectures', 'pro', 'con', 'debates', 'seminars', 'tutor', '##ials', 'high', 'standard', 'meeting', 'marked', 'sense', 'renewed', 'enthusiasm', 'positive', 'progress', 'occurring', 'intensive', 'care', 'medicine']),\n TokenizedText(text_id='8qnrcgnk', tokens=['hem', '##e', 'oxygen', '##ase', '1', 'carbon', 'mono', '##xide', 'pulmonary', 'medicine', 'hem', '##e', 'oxygen', '##ase', '1', 'ho', '1', 'ind', '##ucible', 'stress', 'protein', 'con', '##fers', 'cy', '##top', '##rot', '##ection', 'ox', '##ida', '##tive', 'stress', 'vitro', 'vivo', 'addition', 'physiological', 'role', 'hem', '##e', 'degradation', 'ho', '1', 'may', 'influence', 'number', 'cellular', 'processes', 'including', 'growth', 'inflammation', 'ap', '##op', '##tosis', 'virtue', 'anti', 'inflammatory', 'effects', 'ho', '1', 'limits', 'tissue', 'damage', 'response', 'pro', '##in', '##fl', '##am', '##mat', '##ory', 'stimuli', 'prevents', '##og', '##raf', '##t', 'rejection', 'transplant', '##ation', 'transcription', '##al', '##re', '##gul', '##ation', 'ho', '1', 'responds', 'many', 'agents', 'h', '##yp', '##ox', '##ia', 'bacterial', 'lip', '##op', '##ol', '##ys', '##ac', '##cha', '##ride', 'reactive', 'oxygen', 'nitrogen', 'species', 'ho', '1', 'con', '##sti', '##tu', '##tively', 'expressed', 'iso', '##zy', '##me', 'hem', '##e', 'oxygen', '##ase', '2', 'cat', '##aly', '##ze', 'rate', 'limiting', 'step', 'conversion', 'hem', '##e', 'meta', '##bol', '##ites', 'bi', '##li', '##ru', '##bin', 'ix', '##α', 'fe', '##rro', '##us', 'iron', 'carbon', 'mono', '##xide', 'co', 'mechanisms', 'ho', '1', 'provides', 'protection', 'likely', 'involve', 'en', '##zy', '##matic', 'reaction', 'products', 'remarkably', 'administration', 'co', 'low', 'concentrations', 'substitute', 'ho', '1', 'respect', 'anti', 'inflammatory', 'anti', 'ap', '##op', '##to', '##tic', 'effects', 'suggesting', 'role', 'co', 'key', 'media', '##tor', 'ho', '1', 'function', 'chronic', 'low', 'level', 'ex', '##ogen', '##ous', 'exposure', 'co', 'cigarette', 'smoking', 'contributes', 'importance', 'co', 'pulmonary', 'medicine', 'implications', 'ho', '1', 'co', 'system', 'pulmonary', 'diseases', 'discussed', 'review', 'emphasis', 'inflammatory', 'states']),\n TokenizedText(text_id='jg13scgo', tokens=['technical', 'description', 'rods', 'real', 'time', 'public', 'health', 'surveillance', 'system', 'report', 'describes', 'design', 'implementation', 'real', 'time', 'outbreak', 'disease', 'surveillance', 'rods', 'system', 'computer', 'based', 'public', 'health', 'surveillance', 'system', 'early', 'detection', 'disease', 'outbreak', '##s', 'hospitals', 'send', 'rods', 'data', 'clinical', 'encounters', 'virtual', 'private', 'networks', 'leased', 'lines', 'using', 'health', 'level', '7', 'h', '##l', '##7', 'message', 'protocol', 'data', 'sent', 'real', 'time', 'rods', 'automatically', 'class', '##ifies', 'registration', 'chief', 'complaint', 'visit', 'one', 'seven', 'syndrome', 'categories', 'using', 'bay', '##esian', 'class', '##ifiers', 'stores', 'data', 'relational', 'database', 'aggregate', '##s', 'data', 'analysis', 'using', 'data', 'ware', '##ho', '##using', 'techniques', 'applies', 'un', '##ivar', '##iate', 'multi', '##var', '##iate', 'statistical', 'detection', 'algorithms', 'data', 'alert', '##s', 'users', 'algorithms', 'identify', '##oma', '##lous', 'patterns', 'syndrome', 'counts', 'rods', 'also', 'web', 'based', 'user', 'interface', 'supports', 'temporal', 'spatial', 'analyses', 'rods', 'processes', 'sales', 'counter', 'health', 'care', 'products', 'similar', 'manner', 'receives', 'data', 'batch', 'mode', 'daily', 'basis', 'rods', 'used', '2002', 'winter', 'olympics', 'currently', 'operates', 'two', 'states', '—', 'pennsylvania', 'utah', 'continues', 'resource', 'implementing', 'evaluating', 'applying', 'new', 'methods', 'public', 'health', 'surveillance'])]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(DOCS_TOKENS_PATH):\n",
    "    tokenized_docs: tkn.TokenizedDocuments = tkn.get_tokenized_documents(documents)\n",
    "\n",
    "    with open(DOCS_TOKENS_PATH, \"wb\") as f:\n",
    "        pickle.dump(tokenized_docs, f)\n",
    "else:\n",
    "    with open(DOCS_TOKENS_PATH, \"rb\") as f:\n",
    "        tokenized_docs: tkn.TokenizedDocuments = pickle.load(f)\n",
    "\n",
    "tokenized_docs[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing queries: 100%|██████████| 50/50 [00:00<00:00, 1509.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[TokenizedText(text_id='1', tokens=['origin', 'co', '##vid', '19']),\n TokenizedText(text_id='2', tokens=['corona', '##virus', 'respond', 'changes', 'weather']),\n TokenizedText(text_id='3', tokens=['sar', '##s', 'co', '##v', '##2', 'infected', 'people', 'develop', 'immunity', 'cross', 'protection', 'possible']),\n TokenizedText(text_id='4', tokens=['causes', 'death', 'co', '##vid', '19']),\n TokenizedText(text_id='5', tokens=['drugs', 'active', 'sar', '##s', 'co', '##v', 'sar', '##s', 'co', '##v', '2', 'animal', 'studies']),\n TokenizedText(text_id='6', tokens=['types', 'rapid', 'testing', 'co', '##vid', '19', 'developed']),\n TokenizedText(text_id='7', tokens=['ser', '##ological', 'tests', 'detect', 'antibodies', 'corona', '##virus']),\n TokenizedText(text_id='8', tokens=['lack', 'testing', 'availability', 'led', '##re', '##port', '##ing', 'true', 'incidence', 'co', '##vid', '19']),\n TokenizedText(text_id='9', tokens=['co', '##vid', '19', 'affected', 'canada']),\n TokenizedText(text_id='10', tokens=['social', 'di', '##stan', '##cing', 'impact', 'slowing', 'spread', 'co', '##vid', '19'])]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(QUERY_TOKENS_PATH):\n",
    "    tokenized_queries: tkn.TokenizedQueries = tkn.get_tokenized_queries(queries)\n",
    "\n",
    "    with open(QUERY_TOKENS_PATH, \"wb\") as f:\n",
    "        pickle.dump(tokenized_queries, f)\n",
    "else:\n",
    "    with open(QUERY_TOKENS_PATH, \"rb\") as f:\n",
    "        tokenized_queries: tkn.TokenizedQueries = pickle.load(f)\n",
    "\n",
    "tokenized_queries[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These two maps are used to map the raw numpy array indices to their actual text ids in the original dataset.\n",
    "This is useful to later retrieve the original text\n",
    "\n",
    "These index-to-ID mappings can be useful for looking up documents or queries by their index in a list or array, rather than by their text ID. For example, if you have a list of relevance scores for each query-document pair, you could use these mappings to look up the text IDs of the corresponding queries and documents based on their index in the relevance score list."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "doc_idx_to_id = {\n",
    "    i: d.text_id for i, d in enumerate(tokenized_docs)\n",
    "}\n",
    "\n",
    "query_idx_to_id = {\n",
    "    i: q.text_id for i, q in enumerate(tokenized_queries)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embeddings & Document Scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nquery_id: (dense, sparse)\\n'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BM25_SCORER = bm25.BM25Okapi(corpus=[d.tokens for d in tokenized_docs])\n",
    "\n",
    "SCORES_DIR_PATH = os.path.join(data_path, \"scores\")\n",
    "\n",
    "scores_by_query_id: Dict[str, eva.ScoresPair] = {}\n",
    "\"\"\"\n",
    "query_id: (dense, sparse)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "if not os.path.exists(SCORES_DIR_PATH):\n",
    "    os.mkdir(SCORES_DIR_PATH)\n",
    "\n",
    "    for q_idx, q_text in enumerate(queries.values()):\n",
    "        dense = scr.get_dense_embeddings_scores(query=q_text, docs=documents)\n",
    "        sparse = scr.get_sparse_embeddings_scores(query=tokenized_queries[q_idx], scorer=BM25_SCORER)\n",
    "        scores_by_query_id[q_idx] = eva.ScoresPair(dense, sparse)\n",
    "\n",
    "        q_id = query_idx_to_id[q_idx]\n",
    "        np.savetxt(os.path.join(SCORES_DIR_PATH, f\"dense_scores_{q_id}.np\"), dense, delimiter=\",\")\n",
    "        np.savetxt(os.path.join(SCORES_DIR_PATH, f\"sparse_scores_{q_id}.np\"), sparse, delimiter=\",\")\n",
    "else:\n",
    "    for q_id in queries.keys():\n",
    "        sparse_path = os.path.join(SCORES_DIR_PATH, f\"sparse_scores_{q_id}.np\")\n",
    "        dense_path = os.path.join(SCORES_DIR_PATH, f\"dense_scores_{q_id}.np\")\n",
    "\n",
    "        # I might have decided to not examine all the queries to save time\n",
    "        if not os.path.exists(sparse_path) or not os.path.exists(dense_path):\n",
    "            break\n",
    "\n",
    "        sparse = np.genfromtxt(sparse_path, delimiter=\",\")\n",
    "        dense = np.genfromtxt(dense_path, delimiter=\",\")\n",
    "        scores_by_query_id[q_id] = eva.ScoresPair(dense, sparse)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "K_VALUES = [5, 10, 25, 50, 100, 250, 500, 1000, 2500, 5000]\n",
    "STEP = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RESULTS_DIR = os.path.join(data_path, \"results\")\n",
    "RESULTS_FILE_PATH = os.path.join(RESULTS_DIR, \"results.pkl\")\n",
    "if not os.path.exists(RESULTS_FILE_PATH):\n",
    "    os.mkdir(RESULTS_DIR)\n",
    "\n",
    "    results_by_k: eva.ResultsByK = eva.get_dataset_results(\n",
    "        scores_by_query_id=scores_by_query_id,\n",
    "        k_values=K_VALUES,\n",
    "        idx_to_doc_id=doc_idx_to_id\n",
    "    )\n",
    "\n",
    "    with(RESULTS_FILE_PATH, \"wb\") as f:\n",
    "        pickle.dump(results_by_k, f)\n",
    "else:\n",
    "    with(RESULTS_FILE_PATH, \"rb\") as f:\n",
    "        results_by_k: eva.ResultsByK = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list(results_by_k.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Template (TODO later remove)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "def get_dataset():\n",
    "  # Caricare dataset da file cartella. Cos'è? array numpy? dict? pandas DataFrame?\n",
    "  queries = get_query_vectors()\n",
    "  documents = get_document_vectors()\n",
    "  pass\n",
    "\n",
    "def get_query_vectors():\n",
    "  pass\n",
    "\n",
    "def get_document_vectors():\n",
    "  pass\n",
    "\n",
    "def tokenize(word_vector):\n",
    "  # Gli si passa un vettore di parole (oppure frasi) e si tokenizza\n",
    "  pass\n",
    "\n",
    "# Vettori densi e sparsi sono prodotte in funzione di (query singola, tutti i documenti)\n",
    "\n",
    "def get_dense_vector(query_token_vector, document_token_vectors):\n",
    "  # Qui si usa il modello pre-allenato\n",
    "  pass\n",
    "\n",
    "def get_sparse_vector(query_token_vector, document_token_vectors):\n",
    "  # Si produce un vettore sparso usando bm25 o tf-idf\n",
    "  pass\n",
    "\n",
    "def inner_product(*args):\n",
    "  # qui sarebbe queryxquery, docxdoc\n",
    "  # In particolare, densexdense + sparsexsparse\n",
    "  pass\n",
    "\n",
    "def get_top_k(score_vector):\n",
    "  # Ritorna i top k del vettore di score\n",
    "  pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_ground_truth(query_vector, doc_vectors, k):\n",
    "  # Qui è top k preso dopo inner product su tutto (step 4 foto)\n",
    "  pass\n",
    "\n",
    "def get_approximate_set(query_vector, doc_vectors, k, k_prime):\n",
    "  # Calcola versioni dense e sparse, però estrai i top k_prime separatamente\n",
    "  #   da densi e sparsi e poi fai unione\n",
    "  # Togliere doppioni\n",
    "  # Eseguire ground truth procedure su approximate set per ottenere esattamente k elementi\n",
    "  pass\n",
    "\n",
    "def calculate_recall(ground_truth, approximate_set):\n",
    "  pass\n",
    "\n",
    "# Fare questo per k' diversi e poi plottare grafico dei recall"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "sbruch-assignment-lwmd",
   "language": "python",
   "display_name": "lwmd_asgmt_sbruch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
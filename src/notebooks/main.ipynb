{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb324e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T23:29:48.780128Z",
     "start_time": "2023-04-21T23:29:12.474278Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rank_bm25 as bm25\n",
    "from typing import Tuple, Dict\n",
    "from beir import util, LoggingHandler\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "\n",
    "import logging\n",
    "\n",
    "# Followed beir tutorial to download dataset:\n",
    "# https://github.com/beir-cellar/beir#beers-quick-example\n",
    "\n",
    "# Show library logs in stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "\n",
    "DATASET_NAME = \"trec-covid\"\n",
    "DATASET_URL = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{DATASET_NAME}.zip\"\n",
    "DATA_DIR = \"../../data\"\n",
    "data_path = util.download_and_unzip(DATASET_URL, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d0a80",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Some typedefs to better grasp how the dataset is actually structured\n",
    "\n",
    "GroundTruth = Dict[str, Dict[str, int]]\n",
    "\"\"\"\n",
    "Example::\n",
    "\n",
    "  {\n",
    "    query_id: {\n",
    "      doc_id: score,\n",
    "    },\n",
    "  }\n",
    "\n",
    "Note: this won't be used in actuality because we will calculate our own ground truth\n",
    "\"\"\"\n",
    "\n",
    "Queries = Dict[str, str]\n",
    "\"\"\"\n",
    "Example::\n",
    "\n",
    "  {\n",
    "    query_id: text,\n",
    "  }\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Corpus = Dict[str, Dict[str, str]]\n",
    "\"\"\"\n",
    "Example::\n",
    "\n",
    "  {\n",
    "    doc_id: {\n",
    "      title: <title>\n",
    "      text: <text>\n",
    "    },\n",
    "  }\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Dataset = Tuple[Corpus, Queries, GroundTruth]\n",
    "\n",
    "# Load corpus, queries and ground truth from the downloaded dataset.\n",
    "# Ground truth here is referred to as `_` because we do not care about it,\n",
    "#   part of the assignment is to calculate our own ground truth\n",
    "data: Dataset = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "corpus, queries, _ = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aab02ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T22:37:23.140072Z",
     "start_time": "2023-04-21T22:37:23.056424Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "  # Caricare dataset da file cartella. Cos'è? array numpy? dict? pandas DataFrame?\n",
    "  queries = get_query_vectors()\n",
    "  documents = get_document_vectors()\n",
    "  pass\n",
    "\n",
    "def get_query_vectors():\n",
    "  pass\n",
    "\n",
    "def get_document_vectors():\n",
    "  pass\n",
    "\n",
    "def tokenize(word_vector):\n",
    "  # Gli si passa un vettore di parole (oppure frasi) e si tokenizza\n",
    "  pass\n",
    "\n",
    "# Vettori densi e sparsi sono prodotte in funzione di (query singola, tutti i documenti)\n",
    "\n",
    "def get_dense_vector(query_token_vector, document_token_vectors):\n",
    "  # Qui si usa il modello pre-allenato\n",
    "  pass\n",
    "\n",
    "def get_sparse_vector(query_token_vector, document_token_vectors):\n",
    "  # Si produce un vettore sparso usando bm25 o tf-idf\n",
    "  pass\n",
    "\n",
    "def inner_product(*args):\n",
    "  # qui sarebbe queryxquery, docxdoc\n",
    "  # In particolare, densexdense + sparsexsparse\n",
    "  pass\n",
    "\n",
    "def get_top_k(score_vector):\n",
    "  # Ritorna i top k del vettore di score\n",
    "  pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_ground_truth(query_vector, doc_vectors, k):\n",
    "  # Qui è top k preso dopo inner product su tutto (step 4 foto)\n",
    "  pass\n",
    "\n",
    "def get_approximate_set(query_vector, doc_vectors, k, k_prime):\n",
    "  # Calcola versioni dense e sparse, però estrai i top k_prime separatamente\n",
    "  #   da densi e sparsi e poi fai unione\n",
    "  # Togliere doppioni\n",
    "  # Eseguire ground truth procedure su approximate set per ottenere esattamente k elementi\n",
    "  pass\n",
    "\n",
    "def calculate_recall(ground_truth, approximate_set):\n",
    "  pass\n",
    "\n",
    "# Fare questo per k' diversi e poi plottare grafico dei recall\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python [conda env:sbruch-assignment-3]",
   "language": "python",
   "name": "conda-env-sbruch-assignment-3-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}